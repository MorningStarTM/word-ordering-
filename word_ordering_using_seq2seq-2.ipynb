{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX8xwX_dEgTN"
   },
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YtJ5Iywdz8uo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HpoMAfAIdrBP"
   },
   "outputs": [],
   "source": [
    "#read the data\n",
    "dt = pd.read_csv(\"./data/word_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YYWDQixcAqnt"
   },
   "outputs": [],
   "source": [
    "or_cor = []\n",
    "un_cor = []\n",
    "for i in range(len(dt['Ordered'])):\n",
    "  text = dt['Ordered'][i].replace(\".\",\"\")\n",
    "  if len(text.split()) == 3:\n",
    "    or_cor.append(text)\n",
    "  text = dt['Unordered'][i]\n",
    "  if len(text.split()) == 3:\n",
    "    un_cor.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wosUCupSBRBt"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-sYmGofPCGtP"
   },
   "outputs": [],
   "source": [
    "df['Ordered'] = or_cor\n",
    "df['Unordered'] = un_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yz7f1avWCTBN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ordered</th>\n",
       "      <th>Unordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love dogs</td>\n",
       "      <td>I dogs love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She plays tennis</td>\n",
       "      <td>She plays tennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They drink coffee</td>\n",
       "      <td>drink coffee They</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He eats pizza</td>\n",
       "      <td>pizza He eats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We go shopping</td>\n",
       "      <td>shopping We go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ordered          Unordered\n",
       "0        I love dogs        I dogs love\n",
       "1   She plays tennis   She plays tennis\n",
       "2  They drink coffee  drink coffee They\n",
       "3      He eats pizza      pizza He eats\n",
       "4     We go shopping     shopping We go"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gy8iSYCmvLqj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1038, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MOa8FbUdh-Ns"
   },
   "outputs": [],
   "source": [
    "#list for words and text\n",
    "input_words = []\n",
    "input_text = []\n",
    "target_words = []\n",
    "target_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cmUkKmdnd7rS"
   },
   "outputs": [],
   "source": [
    "#split the unordered text and append into list\n",
    "for i in range(len(df)):\n",
    "    data = df['Unordered'][i]\n",
    "    data = data.lower()\n",
    "    word_list = data.split(' ')\n",
    "    if len(word_list) <= 3:\n",
    "      input_text.append(word_list)\n",
    "    for word in word_list:\n",
    "      if word not in input_words:\n",
    "        input_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i-Qt6Vc85pQT"
   },
   "outputs": [],
   "source": [
    "#split the ordered text and append into list\n",
    "for i in range(len(df)):\n",
    "    data = df['Ordered'][i]\n",
    "    data = data.lower()\n",
    "    word_list = data.split(' ')\n",
    "    if len(word_list) <= 3:\n",
    "      target_text.append(word_list)\n",
    "    for word in word_list:\n",
    "      if word not in target_words:\n",
    "        target_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ed2vGy0A1ASP"
   },
   "outputs": [],
   "source": [
    "num_encoders_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "irooWmVW1Sps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoders_tokens : 337\n",
      "num_decoder_tokens : 337\n"
     ]
    }
   ],
   "source": [
    "print(f'num_encoders_tokens : {num_encoders_tokens}')\n",
    "print(f'num_decoder_tokens : {num_decoder_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GPzL5vg6ibRa"
   },
   "outputs": [],
   "source": [
    "#sort the list\n",
    "input_words = sorted(list(input_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y9v0zKGtCuSV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Pe9_nAFvjRZ6"
   },
   "outputs": [],
   "source": [
    "vocabulary = len(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DKTUDTyqjd8v"
   },
   "outputs": [],
   "source": [
    "#assign the index for each word\n",
    "input_word_index = dict(\n",
    "    (char, i) for i, char in enumerate(input_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EaxGARRTBrts"
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict(\n",
    "    (i, char) for i, char in enumerate(input_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DTVfhuhUkXf-"
   },
   "outputs": [],
   "source": [
    "#create the vector for sentence\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_text), 3, 337), dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_text), 3, 337), dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_text), 3, 337), dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fb3uURtamPCr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1038, 3, 337)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ar1Ov3U4sN-L"
   },
   "outputs": [],
   "source": [
    "#preparing vector \n",
    "for j in range(len(input_text)):\n",
    "  for i in range(len(input_text[j])):\n",
    "    value = input_word_index[input_text[j][i]]\n",
    "    encoder_input_data[j][i][value] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BFiyg8lX69z0"
   },
   "outputs": [],
   "source": [
    "#preparing vector \n",
    "for j in range(len(target_text)):\n",
    "  for i in range(len(target_text[j])):\n",
    "    value = input_word_index[target_text[j][i]]\n",
    "    decoder_input_data[j][i][value] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SO69nLUXsbtq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('abroad', 0), ('animations', 1), ('anthropology', 2), ('appliances', 3), ('archaeology', 4), ('archery', 5), ('art', 6), ('articles', 7), ('astronomy', 8), ('attend', 9), ('attends', 10), ('attentively', 11), ('bake', 12), ('bakes', 13), ('ballet', 14), ('baseball', 15), ('basketball', 16), ('beautifully', 17), ('bicycles', 18), ('bike', 19), ('bikes', 20), ('biking', 21), ('biology', 22), ('birds', 23), ('birdwatching', 24), ('birthdays', 25), ('blogs', 26), ('boardgames', 27), ('books', 28), ('botany', 29), ('boxing', 30), ('breakfast', 31), ('breaks', 32), ('build', 33), ('buildings', 34), ('builds', 35), ('business', 36), ('cakes', 37), ('calligraphy', 38), ('camping', 39), ('canoeing', 40), ('cards', 41), ('care', 42), ('carefully', 43), ('cars', 44), ('cartoons', 45), ('cats', 46), ('celebrate', 47), ('chemistry', 48), ('chess', 49), ('chocolate', 50), ('chores', 51), ('church', 52), ('cities', 53), ('classes', 54), ('clean', 55), ('climb', 56), ('clothes', 57), ('clothing', 58), ('code', 59), ('codes', 60), ('coding', 61), ('coffee', 62), ('coins', 63), ('collect', 64), ('comics', 65), ('computers', 66), ('concerts', 67), ('conferences', 68), ('cook', 69), ('cooking', 70), ('cooks', 71), ('courses', 72), ('crafts', 73), ('create', 74), ('creates', 75), ('crosswords', 76), ('cycling', 77), ('daily', 78), ('dance', 79), ('dances', 80), ('dancing', 81), ('deeply', 82), ('designs', 83), ('dinner', 84), ('dishes', 85), ('do', 86), ('documentaries', 87), ('does', 88), ('dogs', 89), ('draw', 90), ('drawing', 91), ('draws', 92), ('drink', 93), ('drives', 94), ('drums', 95), ('eat', 96), ('eats', 97), ('ecology', 98), ('economics', 99), ('education', 100), ('emails', 101), ('engineering', 102), ('english', 103), ('enjoy', 104), ('enjoys', 105), ('errands', 106), ('essays', 107), ('events', 108), ('everyday', 109), ('exercise', 110), ('exercises', 111), ('explore', 112), ('family', 113), ('fast', 114), ('festivals', 115), ('fireworks', 116), ('fishing', 117), ('fixes', 118), ('flowers', 119), ('football', 120), ('friends', 121), ('frisbee', 122), ('fun', 123), ('games', 124), ('gardening', 125), ('gardens', 126), ('geography', 127), ('geology', 128), ('go', 129), ('goes', 130), ('golf', 131), ('gracefully', 132), ('grows', 133), ('guitar', 134), ('gymnastics', 135), ('hard', 136), ('have', 137), ('he', 138), ('hike', 139), ('hiking', 140), ('history', 141), ('hockey', 142), ('holidays', 143), ('homework', 144), ('horses', 145), ('house', 146), ('houses', 147), ('housework', 148), ('i', 149), ('jewelry', 150), ('jogging', 151), ('journals', 152), ('karaoke', 153), ('karate', 154), ('kayaking', 155), ('knitting', 156), ('landscapes', 157), ('languages', 158), ('laps', 159), ('laughs', 160), ('laundry', 161), ('law', 162), ('learn', 163), ('learns', 164), ('lectures', 165), ('letters', 166), ('like', 167), ('linguistics', 168), ('listen', 169), ('listens', 170), ('literature', 171), ('logos', 172), ('loudly', 173), ('love', 174), ('loves', 175), ('lunch', 176), ('magazines', 177), ('magic', 178), ('make', 179), ('makes', 180), ('makeup', 181), ('marathons', 182), ('math', 183), ('mathematics', 184), ('meals', 185), ('medicine', 186), ('meditate', 187), ('meditates', 188), ('meditation', 189), ('meetings', 190), ('mindfulness', 191), ('motorcycles', 192), ('mountains', 193), ('movies', 194), ('museums', 195), ('music', 196), ('naps', 197), ('nature', 198), ('newspapers', 199), ('notes', 200), ('novels', 201), ('often', 202), ('online', 203), ('out', 204), ('outside', 205), ('paint', 206), ('painting', 207), ('paints', 208), ('parties', 209), ('people', 210), ('perform', 211), ('philosophy', 212), ('photographs', 213), ('photography', 214), ('photos', 215), ('physics', 216), ('piano', 217), ('picnics', 218), ('pictures', 219), ('pilates', 220), ('pizza', 221), ('planes', 222), ('plant', 223), ('plants', 224), ('play', 225), ('plays', 226), ('poems', 227), ('poetry', 228), ('politics', 229), ('portraits', 230), ('pottery', 231), ('practice', 232), ('practices', 233), ('psychology', 234), ('push-ups', 235), ('puzzles', 236), ('quickly', 237), ('quilts', 238), ('read', 239), ('reads', 240), ('regularly', 241), ('religion', 242), ('remotely', 243), ('ride', 244), ('robots', 245), ('rollercoasters', 246), ('rugby', 247), ('run', 248), ('running', 249), ('runs', 250), ('safely', 251), ('sailing', 252), ('sandcastles', 253), ('science', 254), ('scripts', 255), ('sculpting', 256), ('selfies', 257), ('sells', 258), ('seminars', 259), ('sews', 260), ('she', 261), ('shopping', 262), ('sightseeing', 263), ('sing', 264), ('singing', 265), ('sings', 266), ('skateboarding', 267), ('skating', 268), ('sketches', 269), ('skiing', 270), ('snorkeling', 271), ('snowboarding', 272), ('soap', 273), ('soccer', 274), ('sociology', 275), ('software', 276), ('solve', 277), ('solves', 278), ('songs', 279), ('spanish', 280), ('speak', 281), ('speaks', 282), ('sports', 283), ('stamps', 284), ('stars', 285), ('statistics', 286), ('stories', 287), ('studies', 288), ('study', 289), ('sunrises', 290), ('sunsets', 291), ('surf', 292), ('swim', 293), ('swimming', 294), ('swims', 295), ('take', 296), ('takes', 297), ('teaches', 298), ('technology', 299), ('tennis', 300), ('theater', 301), ('they', 302), ('thinks', 303), ('time', 304), ('together', 305), ('trails', 306), ('trains', 307), ('travel', 308), ('trees', 309), ('trumpet', 310), ('tv', 311), ('vacations', 312), ('vegetables', 313), ('videos', 314), ('visit', 315), ('visits', 316), ('volleyball', 317), ('volunteer', 318), ('volunteers', 319), ('walk', 320), ('walks', 321), ('watch', 322), ('watches', 323), ('waves', 324), ('we', 325), ('websites', 326), ('weddings', 327), ('wildlife', 328), ('work', 329), ('works', 330), ('workshops', 331), ('write', 332), ('writes', 333), ('writing', 334), ('yoga', 335), ('zoology', 336)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "the7-GfMCIL8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'abroad'), (1, 'animations'), (2, 'anthropology'), (3, 'appliances'), (4, 'archaeology'), (5, 'archery'), (6, 'art'), (7, 'articles'), (8, 'astronomy'), (9, 'attend'), (10, 'attends'), (11, 'attentively'), (12, 'bake'), (13, 'bakes'), (14, 'ballet'), (15, 'baseball'), (16, 'basketball'), (17, 'beautifully'), (18, 'bicycles'), (19, 'bike'), (20, 'bikes'), (21, 'biking'), (22, 'biology'), (23, 'birds'), (24, 'birdwatching'), (25, 'birthdays'), (26, 'blogs'), (27, 'boardgames'), (28, 'books'), (29, 'botany'), (30, 'boxing'), (31, 'breakfast'), (32, 'breaks'), (33, 'build'), (34, 'buildings'), (35, 'builds'), (36, 'business'), (37, 'cakes'), (38, 'calligraphy'), (39, 'camping'), (40, 'canoeing'), (41, 'cards'), (42, 'care'), (43, 'carefully'), (44, 'cars'), (45, 'cartoons'), (46, 'cats'), (47, 'celebrate'), (48, 'chemistry'), (49, 'chess'), (50, 'chocolate'), (51, 'chores'), (52, 'church'), (53, 'cities'), (54, 'classes'), (55, 'clean'), (56, 'climb'), (57, 'clothes'), (58, 'clothing'), (59, 'code'), (60, 'codes'), (61, 'coding'), (62, 'coffee'), (63, 'coins'), (64, 'collect'), (65, 'comics'), (66, 'computers'), (67, 'concerts'), (68, 'conferences'), (69, 'cook'), (70, 'cooking'), (71, 'cooks'), (72, 'courses'), (73, 'crafts'), (74, 'create'), (75, 'creates'), (76, 'crosswords'), (77, 'cycling'), (78, 'daily'), (79, 'dance'), (80, 'dances'), (81, 'dancing'), (82, 'deeply'), (83, 'designs'), (84, 'dinner'), (85, 'dishes'), (86, 'do'), (87, 'documentaries'), (88, 'does'), (89, 'dogs'), (90, 'draw'), (91, 'drawing'), (92, 'draws'), (93, 'drink'), (94, 'drives'), (95, 'drums'), (96, 'eat'), (97, 'eats'), (98, 'ecology'), (99, 'economics'), (100, 'education'), (101, 'emails'), (102, 'engineering'), (103, 'english'), (104, 'enjoy'), (105, 'enjoys'), (106, 'errands'), (107, 'essays'), (108, 'events'), (109, 'everyday'), (110, 'exercise'), (111, 'exercises'), (112, 'explore'), (113, 'family'), (114, 'fast'), (115, 'festivals'), (116, 'fireworks'), (117, 'fishing'), (118, 'fixes'), (119, 'flowers'), (120, 'football'), (121, 'friends'), (122, 'frisbee'), (123, 'fun'), (124, 'games'), (125, 'gardening'), (126, 'gardens'), (127, 'geography'), (128, 'geology'), (129, 'go'), (130, 'goes'), (131, 'golf'), (132, 'gracefully'), (133, 'grows'), (134, 'guitar'), (135, 'gymnastics'), (136, 'hard'), (137, 'have'), (138, 'he'), (139, 'hike'), (140, 'hiking'), (141, 'history'), (142, 'hockey'), (143, 'holidays'), (144, 'homework'), (145, 'horses'), (146, 'house'), (147, 'houses'), (148, 'housework'), (149, 'i'), (150, 'jewelry'), (151, 'jogging'), (152, 'journals'), (153, 'karaoke'), (154, 'karate'), (155, 'kayaking'), (156, 'knitting'), (157, 'landscapes'), (158, 'languages'), (159, 'laps'), (160, 'laughs'), (161, 'laundry'), (162, 'law'), (163, 'learn'), (164, 'learns'), (165, 'lectures'), (166, 'letters'), (167, 'like'), (168, 'linguistics'), (169, 'listen'), (170, 'listens'), (171, 'literature'), (172, 'logos'), (173, 'loudly'), (174, 'love'), (175, 'loves'), (176, 'lunch'), (177, 'magazines'), (178, 'magic'), (179, 'make'), (180, 'makes'), (181, 'makeup'), (182, 'marathons'), (183, 'math'), (184, 'mathematics'), (185, 'meals'), (186, 'medicine'), (187, 'meditate'), (188, 'meditates'), (189, 'meditation'), (190, 'meetings'), (191, 'mindfulness'), (192, 'motorcycles'), (193, 'mountains'), (194, 'movies'), (195, 'museums'), (196, 'music'), (197, 'naps'), (198, 'nature'), (199, 'newspapers'), (200, 'notes'), (201, 'novels'), (202, 'often'), (203, 'online'), (204, 'out'), (205, 'outside'), (206, 'paint'), (207, 'painting'), (208, 'paints'), (209, 'parties'), (210, 'people'), (211, 'perform'), (212, 'philosophy'), (213, 'photographs'), (214, 'photography'), (215, 'photos'), (216, 'physics'), (217, 'piano'), (218, 'picnics'), (219, 'pictures'), (220, 'pilates'), (221, 'pizza'), (222, 'planes'), (223, 'plant'), (224, 'plants'), (225, 'play'), (226, 'plays'), (227, 'poems'), (228, 'poetry'), (229, 'politics'), (230, 'portraits'), (231, 'pottery'), (232, 'practice'), (233, 'practices'), (234, 'psychology'), (235, 'push-ups'), (236, 'puzzles'), (237, 'quickly'), (238, 'quilts'), (239, 'read'), (240, 'reads'), (241, 'regularly'), (242, 'religion'), (243, 'remotely'), (244, 'ride'), (245, 'robots'), (246, 'rollercoasters'), (247, 'rugby'), (248, 'run'), (249, 'running'), (250, 'runs'), (251, 'safely'), (252, 'sailing'), (253, 'sandcastles'), (254, 'science'), (255, 'scripts'), (256, 'sculpting'), (257, 'selfies'), (258, 'sells'), (259, 'seminars'), (260, 'sews'), (261, 'she'), (262, 'shopping'), (263, 'sightseeing'), (264, 'sing'), (265, 'singing'), (266, 'sings'), (267, 'skateboarding'), (268, 'skating'), (269, 'sketches'), (270, 'skiing'), (271, 'snorkeling'), (272, 'snowboarding'), (273, 'soap'), (274, 'soccer'), (275, 'sociology'), (276, 'software'), (277, 'solve'), (278, 'solves'), (279, 'songs'), (280, 'spanish'), (281, 'speak'), (282, 'speaks'), (283, 'sports'), (284, 'stamps'), (285, 'stars'), (286, 'statistics'), (287, 'stories'), (288, 'studies'), (289, 'study'), (290, 'sunrises'), (291, 'sunsets'), (292, 'surf'), (293, 'swim'), (294, 'swimming'), (295, 'swims'), (296, 'take'), (297, 'takes'), (298, 'teaches'), (299, 'technology'), (300, 'tennis'), (301, 'theater'), (302, 'they'), (303, 'thinks'), (304, 'time'), (305, 'together'), (306, 'trails'), (307, 'trains'), (308, 'travel'), (309, 'trees'), (310, 'trumpet'), (311, 'tv'), (312, 'vacations'), (313, 'vegetables'), (314, 'videos'), (315, 'visit'), (316, 'visits'), (317, 'volleyball'), (318, 'volunteer'), (319, 'volunteers'), (320, 'walk'), (321, 'walks'), (322, 'watch'), (323, 'watches'), (324, 'waves'), (325, 'we'), (326, 'websites'), (327, 'weddings'), (328, 'wildlife'), (329, 'work'), (330, 'works'), (331, 'workshops'), (332, 'write'), (333, 'writes'), (334, 'writing'), (335, 'yoga'), (336, 'zoology')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VSvMhXzYMipR"
   },
   "outputs": [],
   "source": [
    "decoder_target_data = decoder_input_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "74hrS0DpM8bX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 337)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VlXTzvO85RWT"
   },
   "outputs": [],
   "source": [
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3n1-e2FE-1R0"
   },
   "outputs": [],
   "source": [
    "#teacher forcing\n",
    "for i in range(len(decoder_target_data)):\n",
    "  a = np.insert(decoder_target_data[i], 0, np.zeros(337))\n",
    "  a = a[:-337]\n",
    "  a = a.reshape(3, 337)\n",
    "  temp.append(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zqJY8E8x5ass"
   },
   "outputs": [],
   "source": [
    "temp = np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "G-1vbBlh5qBA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[loves she you] -> [she loves you] === [2 1 3] -> [1 2 3]  but decoder takes [0 1 2]\n",
    "temp[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXviiyPN93DG"
   },
   "source": [
    "# Decode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rlhiiElM913s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 174, 89]\n"
     ]
    }
   ],
   "source": [
    "index = [np.where(decoder_input_data[0][0] == 1)[0][0], np.where(decoder_input_data[0][1] == 1)[0][0], np.where(decoder_input_data[0][2] == 1)[0][0]]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Waw5n6EgCP1s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'dogs']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for i in index:\n",
    "  t.append(reverse_word_index[i])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnR1ysIUwYOL"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r51Dq7Dy0j-M"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zEPKmFLS1ky8"
   },
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 16\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lwzyZaKj4pZC"
   },
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA8l6G4p1w2E"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Z5FKMF6f1t5m"
   },
   "outputs": [],
   "source": [
    "# Define decoder sequence\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qMYIQ6l31ziR"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nZFvqnub138u"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "j4GR5XDs18ej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.4293 - accuracy: 0.9293 - val_loss: 0.7033 - val_accuracy: 0.8734\n",
      "Epoch 2/16\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3743 - accuracy: 0.9406 - val_loss: 0.6392 - val_accuracy: 0.8830\n",
      "Epoch 3/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.3234 - accuracy: 0.9490 - val_loss: 0.6513 - val_accuracy: 0.8814\n",
      "Epoch 4/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2830 - accuracy: 0.9598 - val_loss: 0.5631 - val_accuracy: 0.8894\n",
      "Epoch 5/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2428 - accuracy: 0.9631 - val_loss: 0.5403 - val_accuracy: 0.9038\n",
      "Epoch 6/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.2085 - accuracy: 0.9687 - val_loss: 0.5312 - val_accuracy: 0.8910\n",
      "Epoch 7/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.1750 - accuracy: 0.9763 - val_loss: 0.5085 - val_accuracy: 0.9103\n",
      "Epoch 8/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.1485 - accuracy: 0.9843 - val_loss: 0.5014 - val_accuracy: 0.8974\n",
      "Epoch 9/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.1279 - accuracy: 0.9863 - val_loss: 0.4706 - val_accuracy: 0.9022\n",
      "Epoch 10/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.1099 - accuracy: 0.9884 - val_loss: 0.4659 - val_accuracy: 0.9167\n",
      "Epoch 11/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0899 - accuracy: 0.9916 - val_loss: 0.4565 - val_accuracy: 0.9071\n",
      "Epoch 12/16\n",
      "104/104 [==============================] - 1s 14ms/step - loss: 0.0731 - accuracy: 0.9952 - val_loss: 0.4616 - val_accuracy: 0.9087\n",
      "Epoch 13/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0613 - accuracy: 0.9968 - val_loss: 0.4482 - val_accuracy: 0.9054\n",
      "Epoch 14/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0515 - accuracy: 0.9960 - val_loss: 0.4324 - val_accuracy: 0.9119\n",
      "Epoch 15/16\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0390 - accuracy: 0.9972 - val_loss: 0.5181 - val_accuracy: 0.8926\n",
      "Epoch 16/16\n",
      "104/104 [==============================] - 2s 14ms/step - loss: 0.0343 - accuracy: 0.9972 - val_loss: 0.4231 - val_accuracy: 0.9119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18123024520>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=8, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPfdB3F8EAOj"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7fiTXF5QEQq8"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ahTGZYyAETdM"
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "OMj-OgQu2HZb"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_encoders_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_seq = list()\n",
    "    while not stop_condition:\n",
    "\n",
    "        # in a loop\n",
    "        # decode the input to a token/output prediction + required states for context vector\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # convert the token/output prediction to a token/output\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_digit = sampled_token_index\n",
    "        # add the predicted token/output to output sequence\n",
    "        decoded_seq.append(sampled_digit)\n",
    "        \n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_digit == '\\n' or\n",
    "           len(decoded_seq) == 3):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the input target sequence (of length 1) \n",
    "        # with the predicted token/output \n",
    "        target_seq = np.zeros((1, 1, num_encoders_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update input states (context vector) \n",
    "        # with the ouputed states\n",
    "        states_value = [h, c]\n",
    "\n",
    "        # loop back.....\n",
    "        \n",
    "    # when loop exists return the output sequence\n",
    "    return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "m3Oxju8nECzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted= decode_sequence(encoder_input_data[0].reshape(1,3,num_encoders_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "66U12_d3E74T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[149, 174, 89]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "kc_Akd2FFBd2"
   },
   "outputs": [],
   "source": [
    "predicted_text = []\n",
    "encoded_text = []\n",
    "index = [np.where(encoder_input_data[0][0] == 1)[0][0], np.where(encoder_input_data[0][1] == 1)[0][0], np.where(encoder_input_data[0][2] == 1)[0][0]]\n",
    "for i in range(len(predicted)):\n",
    "  predicted_text.append(reverse_word_index[predicted[i]])\n",
    "  encoded_text.append(reverse_word_index[index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "B2ZK4A71FKGa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'dogs', 'love']  --->  ['i', 'love', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "print(encoded_text, \" ---> \", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Wlmfqn1lF1rZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_text = []\n",
    "encoded_text = []\n",
    "sampleNo = 10\n",
    "for i in range(0, sampleNo-1):\n",
    "    predicted = decode_sequence(encoder_input_data[i].reshape(1,3,num_encoders_tokens))\n",
    "    index = [np.where(encoder_input_data[i][0] == 1)[0][0], np.where(encoder_input_data[i][1] == 1)[0][0], np.where(encoder_input_data[i][2] == 1)[0][0]]\n",
    "    predicted_text.append(predicted)\n",
    "    encoded_text.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "_4HrjJLbHjzf"
   },
   "outputs": [],
   "source": [
    "decode_encoded_word = []\n",
    "decode_predicted_word = []\n",
    "for encode in encoded_text:\n",
    "    temp_list = []\n",
    "    for token in range(len(encode)):\n",
    "        temp_list.append(reverse_word_index[encode[token]])\n",
    "    decode_encoded_word.append(temp_list)\n",
    "\n",
    "for encode in predicted_text:\n",
    "    temp_list = []\n",
    "    for token in range(len(encode)):\n",
    "        temp_list.append(reverse_word_index[encode[token]])\n",
    "    decode_predicted_word.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'dogs', 'love']  --->  ['i', 'love', 'dogs']\n",
      "['she', 'plays', 'tennis']  --->  ['she', 'plays', 'tennis']\n",
      "['drink', 'coffee', 'they']  --->  ['they', 'drink', 'coffee']\n",
      "['pizza', 'he', 'eats']  --->  ['he', 'eats', 'pizza']\n",
      "['shopping', 'we', 'go']  --->  ['we', 'go', 'shopping']\n",
      "['gracefully', 'she', 'dances']  --->  ['she', 'dances', 'gracefully']\n",
      "['runs', 'fast', 'he']  --->  ['he', 'runs', 'fast']\n",
      "['they', 'daily', 'swim']  --->  ['they', 'swim', 'daily']\n",
      "['i', 'study', 'hard']  --->  ['i', 'study', 'hard']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(decode_predicted_word)):\n",
    "    print(decode_encoded_word[i], ' ---> ', decode_predicted_word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dark-Devil\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "# save the weights individually\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights != []:\n",
    "        np.savez(f'./assets/{layer.name}.npz', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder sequence\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "loaded_model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='Training_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_encoder_lstm = np.load('./assets/lstm.npz', allow_pickle=True)\n",
    "w_decoder_lstm = np.load('./assets/lstm_1.npz', allow_pickle=True)\n",
    "w_dense = np.load('./assets/dense.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights of the model\n",
    "loaded_model.layers[2].set_weights(w_encoder_lstm['arr_0'])\n",
    "loaded_model.layers[3].set_weights(w_decoder_lstm['arr_0'])\n",
    "loaded_model.layers[4].set_weights(w_dense['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted= decode_sequence(encoder_input_data[0].reshape(1,3,num_encoders_tokens))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
