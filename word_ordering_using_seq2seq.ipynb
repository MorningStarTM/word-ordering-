{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNbulXT3obuXLj3DAXlhYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorningStarTM/word-ordering-/blob/main/word_ordering_using_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtJ5Iywdz8uo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the data\n",
        "df = pd.read_csv(\"/content/text.csv\")"
      ],
      "metadata": {
        "id": "HpoMAfAIdrBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list for words and text\n",
        "input_words = []\n",
        "input_text = []\n",
        "target_words = []\n",
        "target_text = []"
      ],
      "metadata": {
        "id": "MOa8FbUdh-Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the unordered text and append into list\n",
        "for i in range(len(df)):\n",
        "    data = df['Unordered'][i]\n",
        "    data = data.lower()\n",
        "    word_list = data.split(' ')\n",
        "    if len(word_list) <= 3:\n",
        "      input_text.append(word_list)\n",
        "    for word in word_list:\n",
        "      if word not in input_words:\n",
        "        input_words.append(word)"
      ],
      "metadata": {
        "id": "cmUkKmdnd7rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the ordered text and append into list\n",
        "for i in range(len(df)):\n",
        "    data = df['Ordered'][i]\n",
        "    data = data.lower()\n",
        "    word_list = data.split(' ')\n",
        "    if len(word_list) <= 3:\n",
        "      target_text.append(word_list)\n",
        "    for word in word_list:\n",
        "      if word not in target_words:\n",
        "        target_words.append(word)"
      ],
      "metadata": {
        "id": "i-Qt6Vc85pQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the list\n",
        "input_words = sorted(list(input_words))"
      ],
      "metadata": {
        "id": "GPzL5vg6ibRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = len(input_words)"
      ],
      "metadata": {
        "id": "Pe9_nAFvjRZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assign the index for each word\n",
        "input_word_index = dict(\n",
        "    (char, i) for i, char in enumerate(input_words)\n",
        ")"
      ],
      "metadata": {
        "id": "DKTUDTyqjd8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the vector for sentence\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_text), 3, 155), dtype='float32'\n",
        ")\n",
        "\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_text), 3, 155), dtype='float32'\n",
        ")\n",
        "\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_text), 3, 155), dtype='float32'\n",
        ")"
      ],
      "metadata": {
        "id": "DTVfhuhUkXf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3uURtamPCr",
        "outputId": "da8ecc69-ddc2-413d-d766-861db469395b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 3, 155)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing vector #teacher force\n",
        "for j in range(len(input_text)):\n",
        "  for i in range(len(input_text[j])):\n",
        "    value = input_word_index[input_text[j][i]]\n",
        "    encoder_input_data[j][i][value] = 1"
      ],
      "metadata": {
        "id": "Ar1Ov3U4sN-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing vector #teacher force\n",
        "for j in range(len(target_text)):\n",
        "  for i in range(len(target_text[j])):\n",
        "    value = input_word_index[target_text[j][i]]\n",
        "    decoder_input_data[j][i][value] = 1"
      ],
      "metadata": {
        "id": "BFiyg8lX69z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_word_index.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO69nLUXsbtq",
        "outputId": "5f47dd62-67ea-4b2c-b3a5-d5f60ee04d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('america', 0), ('arrests', 1), ('art', 2), ('at', 3), ('bake', 4), ('barking', 5), ('basketball', 6), ('bike', 7), ('bikes', 8), ('birds', 9), ('bloom', 10), ('books', 11), ('boy', 12), ('bright', 13), ('build', 14), ('cakes', 15), ('car', 16), ('cat', 17), ('catches', 18), ('caves', 19), ('chess', 20), ('child', 21), ('climbs', 22), ('coffee', 23), ('cook', 24), ('cooking', 25), ('cooks', 26), ('country', 27), ('daily', 28), ('dance', 29), ('dances', 30), ('david', 31), ('designs', 32), ('dog', 33), ('dogs', 34), ('drink', 35), ('drinks', 36), ('drives', 37), ('early', 38), ('explore', 39), ('fast', 40), ('father', 41), ('fish', 42), ('fishes', 43), ('flies', 44), ('flowers', 45), ('flows', 46), ('fly', 47), ('food', 48), ('friends', 49), ('games', 50), ('gracefully', 51), ('grow', 52), ('grows', 53), ('guitar', 54), ('hannah', 55), ('hard', 56), ('he', 57), ('high', 58), ('history', 59), ('houses', 60), ('i', 61), ('in', 62), ('is', 63), ('king', 64), ('kites', 65), ('letters', 66), ('lifts', 67), ('listens', 68), ('love', 69), ('loves', 70), ('magic', 71), ('makes', 72), ('marathons', 73), ('martial', 74), ('meditates', 75), ('milk', 76), ('moon', 77), ('mother', 78), ('mountains', 79), ('movies', 80), ('murals', 81), ('nature', 82), ('newspapers', 83), ('night', 84), ('often', 85), ('out', 86), ('paints', 87), ('perform', 88), ('photos', 89), ('pictures', 90), ('planes', 91), ('plants', 92), ('play', 93), ('plays', 94), ('podcasts', 95), ('poems', 96), ('police', 97), ('portraits', 98), ('practices', 99), ('programs', 100), ('puzzles', 101), ('reads', 102), ('rides', 103), ('rises', 104), ('river', 105), ('rope', 106), ('run', 107), ('runs', 108), ('salsa', 109), ('selfies', 110), ('she', 111), ('shines', 112), ('sings', 113), ('sitting', 114), ('skips', 115), ('soccer', 116), ('software', 117), ('solves', 118), ('songs', 119), ('speaks', 120), ('spring', 121), ('stars', 122), ('studies', 123), ('study', 124), ('studying', 125), ('sun', 126), ('surf', 127), ('swims', 128), ('takes', 129), ('talks', 130), ('tea', 131), ('teacher', 132), ('teaching', 133), ('the', 134), ('they', 135), ('thief', 136), ('trains', 137), ('travel', 138), ('truth', 139), ('tv', 140), ('twinkle', 141), ('visit', 142), ('walks', 143), ('watch', 144), ('waves', 145), ('websites', 146), ('weights', 147), ('well', 148), ('working', 149), ('works', 150), ('writes', 151), ('yoga', 152), ('you', 153)])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = decoder_input_data.copy()"
      ],
      "metadata": {
        "id": "VSvMhXzYMipR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74hrS0DpM8bX",
        "outputId": "c51cb0a8-3b2b-4ca6-b9e1-20f4ab65c4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 155)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(decoder_target_data)):\n",
        "  a = np.insert(decoder_target_data[i], 0, np.zeros(155))\n",
        "  a = a[:-155]\n",
        "  a = a.reshape(3, 155)\n",
        "  print(a)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n1-e2FE-1R0",
        "outputId": "66c08e1a-0769-4915-b32b-dabadfa42123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPvSQ7E6PYdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}